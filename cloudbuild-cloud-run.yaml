steps:

  # Install dependencies
  - name: 'ghcr.io/dbt-labs/dbt-bigquery:latest'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        echo "Running dbt deps to fetch dependencies..."
        dbt deps --project-dir=/workspace  --vars '{ bronze_dataset: "${_BRONZED_DATASET_NAME}", silver_dataset: "${_SILVER_DATASET_NAME}", gold_dataset: "${_GOLD_DATASET_NAME}"}'

    id: 'dbt-deps'

  # Build DBT image
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'build',
      '-t', '${_REGION}-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/dbt:${COMMIT_SHA}',
      '-t', '${_REGION}-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/dbt:latest',
      '.'
    ]
    id: 'build-image'
    waitFor: ['dbt-deps']

  # Push to Artifact Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'push', 
      '--all-tags',
      '${_REGION}-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/dbt'
    ]
    id: 'push-image'
    waitFor: ['build-image']

  # Deploy the DBT image to Cloud Run
  - name: '${_REGION}-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/dbt:latest'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # Generate DBT documentation in a temporary gcs
        dbt docs generate --project-dir=/workspace --profiles-dir=app/profiles --vars '{ bronze_dataset: "${_BRONZED_DATASET_NAME}", silver_dataset: "${_SILVER_DATASET_NAME}", gold_dataset: "${_GOLD_DATASET_NAME}"}'
        
        # Copy the generated documentation to a target directory
        gsutil rsync -r target/ gs://${_DOCS_BUCKET_NAME}/
    id: 'deploy-docs'
    # S'assure que cette étape ne s'exécute qu'après que l'image a été poussée.
    waitFor: ['push-image']

# Trigger the DAG run, passing the new image as a variable
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # download the current DAG from Cloud Composer
        echo "Downloading current DAG from Cloud Composer..."
        gsutil cp gs://${_COMPOSER_BUCKET_NAME}/dags/dbt_run_dag.py ./dbt_run_dag.py
        
        # Check if the file was downloaded successfully
        if [ ! -f "dbt_run_dag.py" ]; then
          echo "Error: Could not download dbt_run_dag.py from Cloud Composer"
          exit 1
        fi
        
        # Update the image in the DAG file
        echo "Updating DAG with new image"
        sed -i "s#image=\"[^\"]*dbt[^\"]*\"#image=\"${_REGION}-docker.pkg.dev/${_PROJECT_ID}/${_REPO_NAME}/dbt:${COMMIT_SHA}\", cmd=[\"dbt\", \"run\", \"--vars\", \"{'bronze_dataset': '${_BRONZE_DATASET_NAME}', 'silver_dataset': '${_SILVER_DATASET_NAME}', 'gold_dataset': '${_GOLD_DATASET_NAME}'}\"]#g" dbt_run_dag.py
      
        # Check if the sed command was successful
        echo "=== DAG modifications ==="
        grep -n "image=" dbt_run_dag.py || echo "No image lines found"
        
        # Upload the updated DAG back to Cloud Composer
        echo "Uploading updated DAG to Cloud Composer..."
        gsutil cp dbt_run_dag.py gs://${_COMPOSER_BUCKET_NAME}/dags/
        
        echo "DAG update completed successfully!"
    id: 'update-dag'


  # Trigger the DAG run
  - name: 'gcr.io/cloud-builders/gcloud'
    args: [
      'composer', 'environments', 'run',
      'composer-dbt-${_ENVIRONMENT}',
      '--location', '${_REGION}',
      'trigger_dag', '--',
      'dbt_run_dag'
    ]
    id: 'trigger-dag'
    waitFor: ['update-dag']

# Substitutions for the build
substitutions:
  _REGION: 'europe-west1'
  _PROJECT_ID: 'my_project'
  _ENVIRONMENT: 'dev'
  _REPO_NAME: 'dbt-repo-${_ENVIRONMENT}'
  _COMPOSER_BUCKET_NAME: 'composer_bucket-${_ENVIRONMENT}'
  _DOCS_BUCKET_NAME: 'docs_bucket-${_ENVIRONMENT}'
  _BRONZE_DATASET_NAME: 'bronze_dev'
  _SILVER_DATASET_NAME: 'silver_dev'
  _GOLD_DATASET_NAME: 'gold_dev'

# Common environment variables 
options:
  logging: CLOUD_LOGGING_ONLY
  

timeout: '1200s'  # 20 minutes